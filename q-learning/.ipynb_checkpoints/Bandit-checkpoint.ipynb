{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandits = [0.2, 0, -0.2, -5]\n",
    "num_bandits = len(bandits)\n",
    "\n",
    "def pullBandit(bandit):\n",
    "    result = np.random.randn(1)\n",
    "    if (result > bandit):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Forward network\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "selected_action = tf.argmax(weights, 0)\n",
    "\n",
    "# Backward\n",
    "model_reward = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "model_action = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "    \n",
    "# Which weight is responsible for selection?\n",
    "responsible_weight = tf.slice(weights, model_action, [1])\n",
    "loss = -(tf.log(responsible_weight) * model_reward)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "solver = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ftse\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [0. 0. 0. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [-1. -1.  0. 49.]\n",
      "[0.999     0.999     1.        1.0469222]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [-1. -2. -1. 95.]\n",
      "[0.999     0.997999  0.9990011 1.0899937]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [ -2.  -1.   0. 142.]\n",
      "[0.997999  0.999001  1.000002  1.1323096]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [ -2.  -1.  -1. 185.]\n",
      "[0.99800104 0.999001   0.9990031  1.1696824 ]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [ -1.   0.   0. 230.]\n",
      "[0.99900305 1.000002   1.0000051  1.2075546 ]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [ -1.  -1.   2. 275.]\n",
      "[0.99900407 0.99900204 1.0020041  1.2442743 ]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [  0.   0.   2. 321.]\n",
      "[1.000005  1.000004  1.0020041 1.2807212]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [  0.   0.   5. 368.]\n",
      "[1.000005  1.000004  1.0049951 1.3169184]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [ -1.   0.   5. 411.]\n",
      "[0.99900603 1.000005   1.0049961  1.3491842 ]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [  0.   0.   5. 458.]\n",
      "[1.000007  1.000006  1.0049961 1.3835908]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [  0.   0.   5. 506.]\n",
      "[1.0000081 1.000006  1.0049961 1.4178671]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [  0.   0.   5. 552.]\n",
      "[1.0000091 1.0000069 1.0049961 1.4499549]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [  1.   0.   6. 600.]\n",
      "[1.0010091 1.0000069 1.0059911 1.4826974]\n",
      "Action: 1\n",
      "Running reward for the 4 bandits: [  1.   1.   6. 645.]\n",
      "[1.0010102 1.000008  1.0059911 1.5134106]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [  3.   1.   5. 692.]\n",
      "[1.0030072 1.001008  1.004997  1.5435126]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [  3.   2.   4. 740.]\n",
      "[1.0030072 1.002007  1.004002  1.5743097]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [  2.   2.   4. 787.]\n",
      "[1.0020102 1.0020081 1.004002  1.6038918]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [  3.   2.   2. 834.]\n",
      "[1.0030082 1.0020081 1.002009  1.6329381]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [  3.   2.   3. 883.]\n",
      "[1.0030082 1.0020081 1.003007  1.66268  ]\n",
      "The agent thinks bandit 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000 #Set total number of episodes to train agent on.\n",
    "total_reward = np.zeros(num_bandits) #Set scoreboard for bandits to 0.\n",
    "epsilon = 0.1 #Set the chance of taking a random action.\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        #Choose either a random action or one from our network.\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(selected_action)\n",
    "        \n",
    "        reward = pullBandit(bandits[action]) #Get our reward from picking one of the bandits.\n",
    "        \n",
    "        #Update the network.\n",
    "        _, resp, ww = sess.run([solver, responsible_weight, weights],\n",
    "                               feed_dict={model_reward:[reward],\n",
    "                                          model_action:[action]})\n",
    "        \n",
    "        #Update our running tally of scores.\n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print(\"Action: {}\".format(action))\n",
    "            print(\"Running reward for the \" + str(num_bandits) + \" bandits: \" + str(total_reward))\n",
    "            print(ww)\n",
    "        i+=1\n",
    "\n",
    "print(\"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "    print(\"...and it was right!\")\n",
    "else:\n",
    "    print(\"...and it was wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualBandit:\n",
    "    def __init__(self):\n",
    "        self.current_bandit_ = 0\n",
    "        self.bandits_ = np.array([[[0.2,0,-0.0,-5],\n",
    "                                   [0.1,-5,1,0.25],\n",
    "                                   [-5,5,5,5]]])\n",
    "        self.num_bandits_ = self.bandits_.shape[0]\n",
    "        self.num_actions_ = self.bandits_.shape[1]\n",
    "        \n",
    "    def randomize(self):\n",
    "        self.current_bandit_ = np.random.randint(0, self.num_bandits_)\n",
    "        print(self.num_bandits_)\n",
    "        \n",
    "    def pull_arm(self, action):\n",
    "        if np.random.randn(1) > self.bandits_[self.current_bandit_, action]:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    @property\n",
    "    def bandit(self):\n",
    "        return self.current_bandit_\n",
    "    \n",
    "    @property\n",
    "    def num_bandits(self):\n",
    "        return self.num_bandits_\n",
    "    \n",
    "    @property\n",
    "    def num_actions(self):\n",
    "        return self.num_actions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, lr, num_bandits, num_actions):\n",
    "        # forward path\n",
    "        self.input_current_bandit_ = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        state_vec = slim.one_hot_encoding(self.input_current_bandit_, num_bandits)\n",
    "        output = slim.fully_connected(state_vec, num_actions,\n",
    "                                      biases_initializer=None,\n",
    "                                      activation_fn=tf.nn.sigmoid,\n",
    "                                      weights_initializer=tf.ones_initializer())\n",
    "        self.output_ = tf.reshape(output, [-1])\n",
    "        self.selected_action = tf.argmax(self.output_, 0)\n",
    "        \n",
    "        # compute loss\n",
    "        self.reward_ = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "        self.action_ = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        \n",
    "        resonsible_weight = tf.slice(self.output_, self.action_, [1])\n",
    "        self.loss_ = -(tf.log(resonsible_weight) * self.reward_)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.solver_ = optimizer.minimize(self.loss_)\n",
    "        \n",
    "    @property\n",
    "    def net_reward(self):\n",
    "        return self.reward_\n",
    "    \n",
    "    @property\n",
    "    def net_action(self):\n",
    "        return self.action_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'fully_connected/weights:0' shape=(1, 3) dtype=float32_ref>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "bandit = ContextualBandit()\n",
    "agent = Agent(0.0001, bandit.num_bandits, bandit.num_actions)\n",
    "total_episode = 10\n",
    "\n",
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(total_episode):\n",
    "        bandit.randomize()\n",
    "        current_bandit = bandit.bandit\n",
    "        print(bandit.current_bandit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "bandit = ContextualBandit()\n",
    "bandit.randomize()\n",
    "print(bandit.bandit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
