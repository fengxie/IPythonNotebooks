{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandits = [0.2, 0, -0.2, -5]\n",
    "num_bandits = len(bandits)\n",
    "\n",
    "def pullBandit(bandit):\n",
    "    result = np.random.randn(1)\n",
    "    if (result > bandit):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Forward network\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "selected_action = tf.argmax(weights, 0)\n",
    "\n",
    "# Backward\n",
    "model_reward = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "model_action = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "    \n",
    "# Which weight is responsible for selection?\n",
    "responsible_weight = tf.slice(weights, model_action, [1])\n",
    "loss = -(tf.log(responsible_weight) * model_reward)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "solver = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0\n",
      "Running reward for the 4 bandits: [1. 0. 0. 0.]\n",
      "[1. 1. 1. 1.]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [-3. -1.  6.  1.]\n",
      "[0.996999   0.99900216 1.005006   1.001     ]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [-3. -2. 22.  2.]\n",
      "[0.996999   0.99800116 1.0208248  1.001999  ]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [-2. -3. 37.  3.]\n",
      "[0.99800205 0.99699914 1.0354359  1.002997  ]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [-4. -4. 41.  6.]\n",
      "[0.995997  0.9959961 1.041234  1.0059851]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [-4. -3. 56.  8.]\n",
      "[0.995997   0.99700016 1.0555619  1.0079722 ]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [-5. -2. 62.  8.]\n",
      "[0.994993  0.9980032 1.0612512 1.0079722]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [-6.  0. 78.  9.]\n",
      "[0.9939879 1.0000062 1.0743809 1.0089643]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [-8.  0. 88.  9.]\n",
      "[0.9919748 1.0000062 1.0836676 1.0089643]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [-8. -1. 93.  9.]\n",
      "[0.9919748 0.9990062 1.0901273 1.0089643]\n",
      "Action: 0\n",
      "Running reward for the 4 bandits: [-10.  -2.  93.  10.]\n",
      "[0.9909677 0.9980062 1.0892261 1.0099554]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [ -9.  -3. 115.  10.]\n",
      "[0.9909687 0.9970052 1.1083547 1.0099554]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [ -9.  -2. 124.  10.]\n",
      "[0.9909687  0.99800825 1.118253   1.0099554 ]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [ -9.  -2. 137.  11.]\n",
      "[0.9909687  0.99800825 1.1280644  1.0109456 ]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [ -9.  -3. 146.  11.]\n",
      "[0.99097073 0.99700725 1.1377884  1.0109456 ]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [ -7.  -3. 150.  13.]\n",
      "[0.99298793 0.99700725 1.1404382  1.0119348 ]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [ -9.  -3. 153.  14.]\n",
      "[0.99097276 0.99700725 1.143956   1.0139103 ]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [-11.  -3. 168.  17.]\n",
      "[0.98895353 0.99700725 1.1552793  1.0168662 ]\n",
      "Action: 3\n",
      "Running reward for the 4 bandits: [-12.  -3. 184.  18.]\n",
      "[0.98794234 0.99700725 1.1699175  1.0168662 ]\n",
      "Action: 2\n",
      "Running reward for the 4 bandits: [-12.  -3. 180.  20.]\n",
      "[0.98794234 0.99700826 1.1656501  1.0198137 ]\n",
      "The agent thinks bandit 3 is the most promising....\n",
      "...and it was wrong!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000 #Set total number of episodes to train agent on.\n",
    "total_reward = np.zeros(num_bandits) #Set scoreboard for bandits to 0.\n",
    "epsilon = 0.1 #Set the chance of taking a random action.\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        #Choose either a random action or one from our network.\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(selected_action)\n",
    "        \n",
    "        reward = pullBandit(bandits[action]) #Get our reward from picking one of the bandits.\n",
    "        \n",
    "        #Update the network.\n",
    "        _, resp, ww = sess.run([solver, responsible_weight, weights],\n",
    "                               feed_dict={model_reward:[reward],\n",
    "                                          model_action:[action]})\n",
    "        \n",
    "        #Update our running tally of scores.\n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print(\"Action: {}\".format(action))\n",
    "            print(\"Running reward for the \" + str(num_bandits) + \" bandits: \" + str(total_reward))\n",
    "            print(ww)\n",
    "        i+=1\n",
    "\n",
    "print(\"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "    print(\"...and it was right!\")\n",
    "else:\n",
    "    print(\"...and it was wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualBandit:\n",
    "    def __init__(self):\n",
    "        self.current_bandit_ = 0\n",
    "        self.bandits_ = np.array([[0.0,0.2,-0.0,-5],\n",
    "                                  [0.1,-5,1,0.25],\n",
    "                                  [-5,5,5,5]])\n",
    "        self.num_bandits_ = self.bandits_.shape[0]\n",
    "        self.num_actions_ = self.bandits_.shape[1]\n",
    "        \n",
    "    def randomize(self):\n",
    "        self.current_bandit_ = np.random.randint(0, self.num_bandits_)\n",
    "        \n",
    "    def pull_arm(self, action):\n",
    "        if np.random.randn(1) > self.bandits_[self.current_bandit_, action]:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    @property\n",
    "    def bandit(self):\n",
    "        return self.current_bandit_\n",
    "    \n",
    "    @property\n",
    "    def num_bandits(self):\n",
    "        return self.num_bandits_\n",
    "    \n",
    "    @property\n",
    "    def num_actions(self):\n",
    "        return self.num_actions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, lr, num_bandits, num_actions):\n",
    "        # forward path\n",
    "        self.input_current_bandit_ = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        state_vec = slim.one_hot_encoding(self.input_current_bandit_, num_bandits)\n",
    "        output = slim.fully_connected(state_vec, num_actions,\n",
    "                                      biases_initializer=None,\n",
    "                                      activation_fn=tf.nn.sigmoid,\n",
    "                                      weights_initializer=tf.ones_initializer())\n",
    "        self.output_ = tf.reshape(output, [-1])\n",
    "        self.selected_action = tf.argmax(self.output_, 0)\n",
    "        \n",
    "        self.weights_ = tf.trainable_variables()[0]\n",
    "        \n",
    "        # compute loss\n",
    "        self.reward_ = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "        self.action_ = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        \n",
    "        resonsible_weight = tf.slice(self.output_, self.action_, [1])\n",
    "        self.loss_ = -(tf.log(resonsible_weight) * self.reward_)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.solver_ = optimizer.minimize(self.loss_)\n",
    "        \n",
    "    @property\n",
    "    def solver(self):\n",
    "        return self.solver_\n",
    "    \n",
    "    @property\n",
    "    def net_reward(self):\n",
    "        return self.reward_\n",
    "    \n",
    "    @property\n",
    "    def net_action(self):\n",
    "        return self.action_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'fully_connected/weights:0' shape=(3, 4) dtype=float32_ref>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "bandit = ContextualBandit()\n",
    "agent = Agent(0.0001, bandit.num_bandits, bandit.num_actions)\n",
    "total_episode = 10000\n",
    "total_reward = np.zeros([bandit.num_bandits, bandit.num_actions])\n",
    "\n",
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for each of the 3 bandits: [-0.25  0.    0.  ]\n",
      "Mean reward for each of the 3 bandits: [42.   37.5  33.25]\n",
      "Mean reward for each of the 3 bandits: [81.5  77.   69.25]\n",
      "Mean reward for each of the 3 bandits: [119.25 117.5  103.5 ]\n",
      "Mean reward for each of the 3 bandits: [159.   155.25 136.5 ]\n",
      "Mean reward for each of the 3 bandits: [195.75 190.25 172.75]\n",
      "Mean reward for each of the 3 bandits: [238.25 224.   206.5 ]\n",
      "Mean reward for each of the 3 bandits: [276.75 258.25 242.75]\n",
      "Mean reward for each of the 3 bandits: [316.75 296.25 276.25]\n",
      "Mean reward for each of the 3 bandits: [356.5  336.   307.25]\n",
      "Mean reward for each of the 3 bandits: [396.5  369.   343.25]\n",
      "Mean reward for each of the 3 bandits: [428.   410.5  382.25]\n",
      "Mean reward for each of the 3 bandits: [468.   445.25 418.5 ]\n",
      "Mean reward for each of the 3 bandits: [508.   483.   453.25]\n",
      "Mean reward for each of the 3 bandits: [546.5  519.5  487.25]\n",
      "Mean reward for each of the 3 bandits: [587.   551.5  526.25]\n",
      "Mean reward for each of the 3 bandits: [627.5  588.5  557.75]\n",
      "Mean reward for each of the 3 bandits: [669.   625.   590.25]\n",
      "Mean reward for each of the 3 bandits: [708.   659.75 624.5 ]\n",
      "Mean reward for each of the 3 bandits: [745.75 699.75 657.75]\n",
      "The agent thinks action 4 for bandit 1 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 2 for bandit 2 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 1 for bandit 3 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(total_episode):\n",
    "        bandit.randomize()\n",
    "        current_bandit = bandit.bandit\n",
    "\n",
    "        if np.random.rand(1) < 0.1:\n",
    "            action = np.random.randint(bandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(agent.selected_action, feed_dict={agent.input_current_bandit_: [current_bandit]})\n",
    "        \n",
    "        reward = bandit.pull_arm(action)\n",
    "            \n",
    "        # print((current_bandit, action, reward))\n",
    "        _, ww = sess.run([agent.solver, agent.weights_], feed_dict={agent.net_reward: [reward],\n",
    "                                                 agent.net_action: [action],\n",
    "                                                 agent.input_current_bandit_: [current_bandit]\n",
    "                                                })\n",
    "        total_reward[current_bandit, action] += reward\n",
    "        if i % 500 == 0:\n",
    "            print(\"Mean reward for each of the \" + str(bandit.num_bandits) + \" bandits: \" + str(np.mean(total_reward,axis=1)))\n",
    "\n",
    "for a in range(bandit.num_bandits):\n",
    "    print(\"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" + str(a+1) + \" is the most promising....\")\n",
    "    if np.argmax(ww[a]) == np.argmin(bandit.bandits_[a]):\n",
    "        print(\"...and it was right!\")\n",
    "    else:\n",
    "        print(\"...and it was wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "bandit = ContextualBandit()\n",
    "bandit.randomize()\n",
    "print(bandit.bandit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
